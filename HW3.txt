CSE 12 Homework 3
Christopher Nies
A11393577
B00
April 15th, 2014

Part 1A
1. True
2. True
3. False
4. False
5. False
6. True
7. False
8. True
9. True
10. False
11. True
12. True
13. True
14. True
15. False
16. True
17. False
18. True

Part 1B
1. Running Time: O(n)
Explanation: A single loop that runs n/2 times, and has one instruction per run
of the loop, so the equation is n/2, and n/2 = O(n).

2. Running Time: O(log2(n)
Explanation: A single loop that iterates log2(n) times, since it multiplies its
cursor by two everytime it runs. log2(n) = O(log2(n))

3. Running Time: O(n^2)
Explanation: A single nested loop, the body of the inner loop itself executed
n times, and then the outer loop is also run n times. So the resulting equation
is roughly 2n^2 + 2n + 1, this comes from the number of times the inner loop's
body is executed (n^2) plus the number of times the inner loops condition
statement is executed (n^2 + n) plus the number of times the outer loop
condition statement is run (n+1). 2n^2 + 2n + 1 = O(n^2)

4. Running Time: O(n)
Explanation: The first loop (body and condition check) is run 2n+1 times, 
as is the second loop. And (assuming the second loop meant j++ instead of i++)
the second loop has the same run time. The whole code then runs 4n + 2 times, 
and 4n + 2 = O(n)

5. Running Time: O(n)
Explanation: This loop's body runs twice as many times as n, so the whole code
plus the condition check runs at 4n + 1, and 4n + 1 = O(n)

6. Running Time: O(n^2)
Explanation: This loop's body executes n*n times, so the whole code runs every
line 2n^2 + 1 times, and 2n^2+1 = O(n^2)

7. Running Time: O(n^3)
Explanation: Another nested loop. The inner loop's body executes n^3 times,
with it's condition check running n^3 + n times. The outer loop's condition runs
n+1 times, so this code executes 2n^3 + 2n + 1 times, and 2n^3 + 2n + 1 = O(n^3)

8. Running Time: O(n)
Explanation: This inner loop's body executes 1000n times, and the inner loops
condition check runs 10001n times. The outer loop condition runs n + 1 times.
So this code executes 20002n + 1 statements, and 2002n + 1 = O(n)

Part 2
1. Running Time: Omega(n)
Explanation: Making a copy of a list requires to create a new list instance(an
operation that runs independent of it's length) and then adding a copy of each 
node to the tail of the list (which runs at roughly 3 times the number of 
elements, since each element needs to be read from the list, then copied, then
added to the new list at the tail). So this takes roughly 3n + 1 steps to 
perform, and 3n +1 = Omega(n)

2. Running Time: Omega(1)
Explanation: Adding an element to the end of the list is easy in a doubly linked
list, since we have access to the tail node and its previous node, which is the
same amount of information to process regardless of length. We need to link the
old final node to the new one, and link the new one to the tail node (each about
two steps regardless of length), and then increment the length of the list (a 
simple operation). This takes about 10 or so steps, and 10=Omega(1)

3. Runing Time: Omega(1)
Explanation: To remove an element we have to disconnect the node from the head
of the list and the element's next node, each taking one step. Then we link the
head to the elements next which takes another 2 steps. Then we set the removed
node to null. So this takes roughly 5 or so steps, and 5 = Omega(1)

4. Running Time: Omega(1)
Explanation: Since this is a doubly linked list, the process is almost identical
to the above process, the only difference being utilizing the tail node and the
node-to-remove's predecessor, rather than the head and the tail-to-remove's
successor. This takes roughly 5 or so steps regardless of length, and
5 = Omega(1)

5. Running Time: Omega(n)
Explanation: In the worst case, the element does not exist in the list. Assuming
the list is not sorted in any way, we have to linearly search the list for the 
element, looking at every single node, getting it's element, and comparing the 
element to the element to search for. Therefore, each node takes about 3 steps,
and with n nodes, to search would take roughly 3n steps. 3n = Omega(n)

Part 3
Question 1.
A. A UNIX command one could use to get the number of words in a file is wc -w,
which will return the number of words in a file followed by the filename. 
119805 medium-wordlist.txt
Another (weirder) way to do it would be with the "cat -n" command and read the
last numbered line, but that's just weird.

B.
main()

  doLoops(MyLinkedList, wordlist, input file, loop bounds and steps)
    <Set up the table by printing the top bar, to make it look nice>
    <Loop the next method call by the number of times indicated by steps, 
    and increase the number of words to look through by incrementing it by
    increment each time>

      doWork(MyLinkedList, wordlist, input file, amount of words to read, number
       of times to read it)
        <Clear MyLinkedList>
        <Do the following for the number of reps indicated in the parameters, 
        keeping track of totalTime elapsed spent on the work>
          <Add every word in "wordlist" into the cleared MyLinkedList>
          <Create two hashsets to keep track of the good and bad words>
          <Open the Scanner, read through every word in the input file (up to 
          the number of words specified)>
            <trim it's punctuation with the static method "trim>
          <Check the MyLinkedList to see if it contains the word the Scanner i
          just read> 
          <If the word is in the List, add it to the "goodwords" HashSet, 
          otherwise add it to "badwords" and 
          increment their respective counters>
          <Once the document is done being read up to the amount of words, get 
          the elapsed time and add it to totalTime>
          <Repeat until done the specified amount of time>
        <One it is all done, take the average trial time, and return it>i

     <Display the result of the "doWork" test, and repeat with a larger amount 
     of words> 
       
  doLoops(MRUList, wordlist, input file, loop bounds and steps)
    <Same Call Outline As Above, only with the new type of LinkedList>
      doWork(MRUListm wordlist, input file, amount of words to read, number of 
      times to read it)
        trimPunctuation(word to strip)

C.
HashSet is a class in the Java library in the java.util package, and it 
implements the "Set" interface. 
"good" differs from "goodwords.size()" in the sense that good keeps track of the
number of good words in the file (including 
repeats), whereas (since goodwords is a Set, and does not allow repeats) 
goodWords.size() is the number of unique goodwords in the file,
ignoring how many times a particular good word is repeated.

D.
To hazard a guess, the fact that the small-wordlist is sorted i
semi-alphabetically might have a tiny bit of difference, at least in the 
MyLinkedList case. The reason being is that the letter 'a' is one of the most 
common letters in the english language to start off a word. Though, coming in 
first place is the letter 't', which is a fair distance down in the alphabet, so
this may not have any effect at all. This is assuming that the list does 
not use any form of binary search (it doesn't, since I wrote it), in which case 
the sorting would be significant.

Question 2
Wordlist: small-wordlist.txt  Document: pride-and-prejudice.txt
Class: MyLinkedList
=======================================
  1:    5000 words in     396 milliseconds
  2:   10000 words in     767 milliseconds
  3:   15000 words in    1100 milliseconds
  4:   20000 words in    1326 milliseconds
  5:   25000 words in    1571 milliseconds

Wordlist: small-wordlist.txt  Document: pride-and-prejudice.txt
Class: MRUList
=======================================
  1:    5000 words in     195 milliseconds
  2:   10000 words in     250 milliseconds
  3:   15000 words in     377 milliseconds
  4:   20000 words in     496 milliseconds
  5:   25000 words in     647 milliseconds

Question 3
For both lists, doubling the amount of words read did roughly double the
amount of time because the growth rate is roughly linear. At first I
believed that the MRUList time would be quicker than twice the time when
the amount to read was doubled, because it was kind of a "smart list" that
makes more common words more accessible. But then I realized that, in terms of
finding and utilizing the common words, there is hardly a difference between 
5000 words and 10000 words, so while using common words cuts down total time,
it does not change how the algorithm scales with time.

Question 4
[cs12sfl@ieng6-202]:HW3:219$ java CollectionTimer medium-wordlist.txt pride-and-prejudice.txt 2500 1250 4
Wordlist: medium-wordlist.txt  Document: pride-and-prejudice.txt
Class: MyLinkedList
=======================================
  1:    2500 words in    8593 milliseconds
  2:    3750 words in   14459 milliseconds
  3:    5000 words in   19187 milliseconds
  4:    6250 words in   25940 milliseconds

Wordlist: medium-wordlist.txt  Document: pride-and-prejudice.txt
Class: MRUList
=======================================
  1:    2500 words in    1949 milliseconds
  2:    3750 words in    2866 milliseconds
  3:    5000 words in    7926 milliseconds
  4:    6250 words in    9470 milliseconds

Yes, as expected the increase in the size of the word list greatly effects the
real-time performance of the program, because it means that the worst case
(where the word is a "bad word") is about ten times as long as before. As before,
the MRUList performs much faster due to its ability to  quickly access the more
common words.

Question 5
Wordlist: pride-and-prejudice.txt  Document: pride-and-prejudice.txt
Class: MyLinkedList
=======================================
  1:    2500 words in     831 milliseconds
  2:    3750 words in    1441 milliseconds
  3:    5000 words in    1701 milliseconds
  4:    6250 words in    2096 milliseconds

Wordlist: pride-and-prejudice.txt  Document: pride-and-prejudice.txt
Class: MRUList
=======================================
  1:    2500 words in     705 milliseconds
  2:    3750 words in    1182 milliseconds
  3:    5000 words in    1599 milliseconds
  4:    6250 words in    1842 milliseconds

I did a few runs of this, and every time the MRUList and MyLinkedList were
actually closer together than most of the other run throughs, even to the point
that the MRUList was occasionally slower. My best guess for as to why would be
that, since all the words are in the exact same place, that the algorithm in
a regular linked list is extremely quick, since the amount it has to search (at most)
increases by one each time. This makes the MyLinkedList really quick for the 
first part of this list. As for why the MRUList is closer to the MyLinkedList
it's probably because, despite all of the common word being accessible, 
every uncommon word is pushed further away with each successive time, which means that in
these cases it is actually slower than the MyLinkedList. As time goes on though, and 
more and more words become processed, the MRUList once again proves much faster due to
it's quick access of the common words.












